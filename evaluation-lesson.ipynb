{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdc59c5",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "[Course content](https://ds.codeup.com/classification/evaluation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae746ca",
   "metadata": {},
   "source": [
    "**Objective:** Understand and apply various metrics used to evaluate the performance of a classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b01baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5b4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A dataframe which contains predicted values and actual values\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d46e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### View the dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9a6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction  coffee  no coffee\n",
       "actual                       \n",
       "coffee           3          2\n",
       "no coffee        1          2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use a crosstab to count the outcomes\n",
    "pd.crosstab(df.actual,df.prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc331d8",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "The two outcomes in classification are labeled as either **positive** or **negative**. \n",
    "\n",
    "\n",
    "While the designations are arbitrary, they impact how evaluation metrics are interpreted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c5c27",
   "metadata": {},
   "source": [
    "### Evaluation on train, test, and split\n",
    "\n",
    "\n",
    "| Split |  Purpose |\n",
    "| ----------- | :----------- |\n",
    "| Train | Evaluate in-sample performance|\n",
    "| Validate |  Evaluate out of sample performance to tune hyper-parameters |\n",
    "| Test | Evaluate performance of model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a265e7",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A diagram which summarizes the outcomes of a model. \n",
    "\n",
    "\n",
    "\n",
    "| Designation      | Description |\n",
    "| ----------- | ----------- |\n",
    "| True Negative      | Model correctly predicted the negative outcome       |\n",
    "| False Positive   | Model incorrectly predicted the positive outcome        |\n",
    "| False Negative   | Model incorrectly predicted the negative outcome        |\n",
    "| True Positive      | Model correctly predicted the positive outcome       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ba95c",
   "metadata": {},
   "source": [
    "### Confusion Matrix with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e58a9",
   "metadata": {},
   "source": [
    "'coffee' is the positive outcome`\n",
    "\n",
    "'no coffee' is the negative outcome\n",
    "\n",
    "\n",
    "The function `confusion_matrix` returns a 2x2 array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64bc14",
   "metadata": {},
   "source": [
    "### Components of a confusion matrix\n",
    " \n",
    " For a confusion matrix $C$,\n",
    "\n",
    "\n",
    "| Index (row, column)      | Count of |\n",
    "| ----------- | ----------- |\n",
    "| $C_{0,0}$      | True Negatives       |\n",
    "| $C_{1,0}$    |   False Negatives      |\n",
    "| $C_{1,1}$    |   True Positives      |\n",
    "| $C_{0,1}$    |   False Positives      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27569e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.actual,df.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7c974dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.prediction,df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ab7dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Return a confusion matrix for the model's predictions\n",
    "confusion_matrix(df.actual,df.prediction,labels=('no coffee','coffee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5bd17a",
   "metadata": {},
   "source": [
    "c[0,0]: There are 2 True Negatives, where we predicted the people don't like cofee and they really don't\n",
    "\n",
    "c[0:1]: There is 1 False Positive, where we predicted the person likes coffee but they really don't\n",
    "\n",
    "c[1,0]: There are 2 False Negatives, where we predicted those people don't like coffee, but they really do\n",
    "\n",
    "c[1,1]: There are 3 True Positives, that is for 4 people they really do like coffee and we predicted they do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671d000",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22ffe3",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "\n",
    "Accuracy evaluates how many correct predictions (both positive and negative) were made over the total number of predictions. \n",
    "\n",
    "\n",
    "$\\texttt{Accuracy} = \\dfrac{TP + TN}{TP + FP + FN + TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dda2a",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision evaluates how many of the positive predictions were correct.\n",
    "\n",
    "$\\texttt{Precision} = \\dfrac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ec1a6",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall evaluates how the model handled all positive outcomes. \n",
    "\n",
    "$\\texttt{Recall} = \\dfrac{TP}{TP + FN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cd4e9",
   "metadata": {},
   "source": [
    "### Misclassification Rate\n",
    "\n",
    "Misclassification rate concerns how many predictions were incorrect. This accounts for all other outcomes not included in the calculation of accuracy. \n",
    "\n",
    "$\\texttt{Misclassification Rate} = 1 - \\texttt{Accuracy}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2ff35",
   "metadata": {},
   "source": [
    "### Sensitivity (True Positive Rate)\n",
    "\n",
    "\n",
    "$\\texttt{True Positive Rate} = \\dfrac{TP}{TP + FN} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ab289",
   "metadata": {},
   "source": [
    "### Specificity \n",
    "\n",
    "How well does the model predict negative outcomes?\n",
    "\n",
    "\n",
    "$\\texttt{Specificity} = \\dfrac{TN}{FP + TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab1564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19cde0ca",
   "metadata": {},
   "source": [
    "### Negative Predictive Value\n",
    "\n",
    "$\\texttt{NPV} = \\dfrac{TN}{TN + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48169b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1287099b",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "$\\texttt{F1  Score} = 2 * \\dfrac{Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c00ff",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "The baseline is a simple model that is a reference point for the performance of other models. \n",
    "\n",
    "For a classification model, a baseline is often the mode.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b07798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coffee       5\n",
       "no coffee    3\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Find the counts of each outcome\n",
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c631406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the baseline_prediction to be coffee\n",
    "df['baseline_prediction'] = 'coffee'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b1d3d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### Evaluation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30257090",
   "metadata": {},
   "source": [
    "## Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c9a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean of bool values give an accuracy\n",
    "#compare the models predicition to actual\n",
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "model_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb85536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the baseline to actual\n",
    "baseline_accuracy = (df.baseline_prediction == df.actual).mean()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b0182f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.62\n",
      "Baseline accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Accuracy: {model_accuracy:.2f}')\n",
    "print(f'Baseline accuracy: {baseline_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3253203",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27eb3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to positive values ('coffee') for the actual values\n",
    "subset = df[df.actual == 'coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f8ac55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model recall\n",
    "model_recall = (subset.prediction == subset.actual).mean()\n",
    "model_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b977db7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model recall\n",
    "baseline_recall = (subset.baseline_prediction == subset.actual).mean()\n",
    "baseline_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "725bc3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model recall: 60.00%\n",
      "Baseline recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(f'Model recall: {model_recall:.2%}')\n",
    "print(f'Baseline recall: {baseline_recall:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419b8ba",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d9aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#were going to restrict to positive prediction values\n",
    "subset = df[df.prediction == 'coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42e7efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_precision = (subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf8ea85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df.baseline_prediction == 'coffee']\n",
    "baseline_precision = (subset.baseline_prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2e34a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision: 75.00%\n",
      "Baseline precision: 0.62\n"
     ]
    }
   ],
   "source": [
    "# using the % to cast a float to a percentage\n",
    "print(f'Model precision: {model_precision:.2%}')\n",
    "print(f'Baseline precision: {baseline_precision:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a34dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
